{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import math\n",
        "import random"
      ],
      "metadata": {
        "id": "7Z4TC5vl5OlA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(1001)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "WVGu2AkyE2xI"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Dataset"
      ],
      "metadata": {
        "id": "tMaFpTrUm7mt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rnw2wHtNmsu4",
        "outputId": "5a97c3a3-6e02-4e01-93ef-6cc861e2ceac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-28 06:41:34--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-10-28 06:41:34 (22.4 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read text file\n",
        "with open('input.txt', 'r') as file:\n",
        "  text = file.read()\n",
        "text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4Aa2gORc0pzz",
        "outputId": "49aac0e6-22fa-4873-8765-4d09c1b3fea3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbkoSXRz1O_l",
        "outputId": "23cfc786-c9f7-4b40-d00f-e42ed39cd163"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert text to [ids] according to vocab\n",
        "vocab = sorted(list(set(text)))\n",
        "print(vocab)\n",
        "\n",
        "id_map = {c : i for (i,c) in enumerate(vocab)}\n",
        "id_map_reverse = {i : c for (i,c) in enumerate(vocab)}\n",
        "print(id_map)\n",
        "\n",
        "def encode(text):\n",
        "  \"\"\"\n",
        "    [chars] -> [ids]\n",
        "  \"\"\"\n",
        "  return [id_map[c] for c in text]\n",
        "\n",
        "def decode(ids):\n",
        "  \"\"\"\n",
        "    [ids] -> string\n",
        "  \"\"\"\n",
        "  return ''.join(id_map_reverse[id] for id in ids)\n",
        "\n",
        "print(decode(encode(\"hello\")) == \"hello\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZJZX9HF1gnH",
        "outputId": "420117ee-c45d-4918-e9b5-2fedcf93a0cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['f', 'h', 'S', \"'\", 'n', 'V', 'j', 'K', '&', 'T', 'O', 'z', 'i', 'q', 'P', 'M', 'W', 'y', 'L', 'v', 'U', 'b', 'Z', 'R', 'X', 'g', '-', 'd', 'Q', 'x', 'o', 'A', 'a', 'w', ';', 'C', 'r', '3', '!', 'N', ' ', 'E', 't', '\\n', 'H', 'p', ',', 'l', 'I', '$', '?', ':', 'Y', 's', '.', 'D', 'B', 'u', 'e', 'm', 'k', 'J', 'c', 'F', 'G']\n",
            "{'f': 0, 'h': 1, 'S': 2, \"'\": 3, 'n': 4, 'V': 5, 'j': 6, 'K': 7, '&': 8, 'T': 9, 'O': 10, 'z': 11, 'i': 12, 'q': 13, 'P': 14, 'M': 15, 'W': 16, 'y': 17, 'L': 18, 'v': 19, 'U': 20, 'b': 21, 'Z': 22, 'R': 23, 'X': 24, 'g': 25, '-': 26, 'd': 27, 'Q': 28, 'x': 29, 'o': 30, 'A': 31, 'a': 32, 'w': 33, ';': 34, 'C': 35, 'r': 36, '3': 37, '!': 38, 'N': 39, ' ': 40, 'E': 41, 't': 42, '\\n': 43, 'H': 44, 'p': 45, ',': 46, 'l': 47, 'I': 48, '$': 49, '?': 50, ':': 51, 'Y': 52, 's': 53, '.': 54, 'D': 55, 'B': 56, 'u': 57, 'e': 58, 'm': 59, 'k': 60, 'J': 61, 'c': 62, 'F': 63, 'G': 64}\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train, val set\n",
        "train = text[:math.floor(0.9*len(text))]\n",
        "test = text[math.floor(0.9*len(text)):]\n",
        "\n",
        "print(f\"train size: {len(train)}, test size: {len(test)}\")\n",
        "\n",
        "def fetch_batch(mode, batch_size, block_size):\n",
        "  if mode=='train':\n",
        "    dataset = train\n",
        "  else:\n",
        "    dataset = test\n",
        "  batch_indices = [random.randint(0, len(dataset)-(block_size+1)) for _ in range(0, batch_size)]\n",
        "  x = torch.stack([torch.tensor(encode(dataset[index:index+block_size])) for index in batch_indices])\n",
        "  y = torch.stack([torch.tensor(encode(dataset[index+1:index+block_size+1])) for index in batch_indices])\n",
        "  return x,y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esfpe5Wm__XY",
        "outputId": "0de1e97f-c88a-46b9-dfc0-67fc9b904ed7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train size: 1003854, test size: 111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x,y=fetch_batch('train',5,8)\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhWURtW1j85Q",
        "outputId": "3bc45d3e-34de-43e5-a0b6-27a32b01f192"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[40, 59, 32, 53, 42, 58, 36, 46],\n",
            "        [40, 32, 42, 42, 58,  4, 27, 40],\n",
            "        [26,  1, 30, 57, 53, 58, 54, 43],\n",
            "        [59, 32, 60, 58, 40, 32, 40, 62],\n",
            "        [53, 40,  0, 57, 47, 47, 40, 30]])\n",
            "tensor([[59, 32, 53, 42, 58, 36, 46, 40],\n",
            "        [32, 42, 42, 58,  4, 27, 40, 30],\n",
            "        [ 1, 30, 57, 53, 58, 54, 43, 43],\n",
            "        [32, 60, 58, 40, 32, 40, 62, 32],\n",
            "        [40,  0, 57, 47, 47, 40, 30,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model architecture\n",
        "class Bigram(nn.Module):\n",
        "  def __init__(self, n_vocab):\n",
        "    super().__init__()\n",
        "    self.embedding_table = nn.Embedding(n_vocab, n_vocab)\n",
        "    self.criterion = nn.CrossEntropyLoss() # takes logits as input, needs logits as <everything packed up> x #classes and target variables as ids\n",
        "\n",
        "  def forward(self, x, y=None):\n",
        "    logits = self.embedding_table(x)\n",
        "    #print(logits.shape)\n",
        "    n_batches, block_size, n_classes = logits.shape\n",
        "    logits_temp = logits.view(n_batches*block_size, n_classes)\n",
        "    if y == None:\n",
        "      return logits, None\n",
        "\n",
        "    y = y.view(n_batches*block_size)\n",
        "    loss = self.criterion(logits_temp, y)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate_next_token(self, x):\n",
        "    logits,_ = self(x)\n",
        "    # last timestep generated\n",
        "    logits = logits[:,-1,:] # shape = batches, score over vocab size\n",
        "    prob = torch.softmax(logits, dim=1)\n",
        "\n",
        "    # top p sampling\n",
        "    pred = torch.multinomial(prob, num_samples=1)\n",
        "\n",
        "    return pred\n",
        "\n",
        "  def generate(self, x, max_tokens):\n",
        "    for i in range(max_tokens):\n",
        "      next_word = self.generate_next_token(x)\n",
        "      x = torch.cat((x, next_word), dim=1) # increase block size\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "1Y4M5ehX38W-"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model obj + optimizer\n",
        "model = Bigram(len(vocab))\n",
        "# move model to device\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "wMRVzfCs_BvL"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape)\n",
        "print(y.shape)\n",
        "\n",
        "logits, loss = model(x,y)\n",
        "print(logits.shape)\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdNkQzh9mguV",
        "outputId": "43bd33ad-64d9-4636-a05e-188bb786897a"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 8])\n",
            "torch.Size([5, 8])\n",
            "torch.Size([5, 8, 65])\n",
            "4.769946098327637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generation without training\n",
        "x = torch.ones((1,1), dtype=torch.long)\n",
        "decode(model.generate(x,100)[0].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "N8CQIM0W0zN-",
        "outputId": "85974434-da6c-42df-df7a-e3c31ceee5f4"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"hp3Mgpf!nDHjnDynVQaqlyDqrd.,&Sy-GZ??'NDXTMO:U&:kHJrex,sD C?hlkse3?BO:UEnDhzlydMj:cGI;Er&v.MppJv.YmHF.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "n_iters = 10000\n",
        "test_after_iters = 1000\n",
        "for iters in range(0, n_iters):\n",
        "  # fetch random batch of training data tensors\n",
        "  x,y = fetch_batch('train',16, 8)\n",
        "  # move tensors to device\n",
        "  x = x.to(device)\n",
        "  y = y.to(device)\n",
        "\n",
        "  # run on model to get loss\n",
        "  logits, loss = model(x, y)\n",
        "\n",
        "  # backprop on loss\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if iters % test_after_iters == 0:\n",
        "    # train loss\n",
        "    print(f\"train loss: {loss.item()}\")\n",
        "    # val loss\n",
        "    x,y = fetch_batch('test', 16, 8)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    logits, loss = model(x, y)\n",
        "    print(f\"val loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "komisIkr_6QV",
        "outputId": "57927c13-892e-452e-9c02-013de24e5db1"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 4.5500102043151855\n",
            "val loss: 4.616785526275635\n",
            "train loss: 4.497040271759033\n",
            "val loss: 4.395230293273926\n",
            "train loss: 4.1943888664245605\n",
            "val loss: 4.119892597198486\n",
            "train loss: 3.9951870441436768\n",
            "val loss: 3.912943124771118\n",
            "train loss: 3.7846150398254395\n",
            "val loss: 3.699993371963501\n",
            "train loss: 3.4637200832366943\n",
            "val loss: 3.6194820404052734\n",
            "train loss: 3.3314173221588135\n",
            "val loss: 3.510249137878418\n",
            "train loss: 3.3457372188568115\n",
            "val loss: 3.204047679901123\n",
            "train loss: 3.2574288845062256\n",
            "val loss: 2.9902563095092773\n",
            "train loss: 2.944948673248291\n",
            "val loss: 3.0935096740722656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generation after training 10k iters\n",
        "x = torch.ones((1,1), dtype=torch.long)\n",
        "decode(model.generate(x,100)[0].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7v_89QOUybyh",
        "outputId": "0986e39e-44f6-4ca6-9579-cdb1f1e29988"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"hbin jg tDHVETon,'OHoR$$uce;\\nsth.LufrdYILFbrNAObPO h tt\\nfokyopiithktGOXd qLQwkGathow'hVme3GI nD: iuck\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    }
  ]
}